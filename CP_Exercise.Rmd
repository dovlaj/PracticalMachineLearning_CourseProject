---
title: "Predicting successful excercise form and types of errors from accelerometer data"
author: "Vladimir Jovanovic"
date: "Saturday, November 22, 2014"
output: html_document
---

```{r set working directory, echo=FALSE}
setwd("C:\\Users\\D0vla\\Desktop\\Coursera\\Practical Machine Learning\\CP_PML\\PracticalMachineLearning_CourseProject")
```

##Abstract

##Introduction

With the introduction of numerous devices capable of collecting large amounts of data about personal activity, it is becoming increasingly simple to acquire amounts of data sufficient to draw informed conclusions about quantity and quality of everyday motion and exercise.

This assignment analyses data collected from people doing barbell lifts correctly, and by intentionally making common types of mistakes. The task is to successfully predict the manner in which the participants did the exercise.

This assignment is part of Practical Machine Learning course taught by Jeff Leek from Johns Hopkins university, and operated by Coursera.

More information about the topic can be gathered at:
http://groupware.les.inf.puc-rio.br/har

```{r load data, cache = TRUE, echo=FALSE}

if(!file.exists("pml-training.csv"))
{
        fileurl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        download.file(fileurl, destfile="pml-training.csv", method="auto")
}
if(!file.exists("pml-testing.csv"))
{
        fileurl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
        download.file(fileurl, destfile="pml-testing.csv", method="auto")
}

trainingData <- read.csv("pml-training.csv", sep = ',', header = TRUE, na.strings = c("", NA, "#DIV/0!"))
testingData <- read.csv("pml-testing.csv", sep = ',', header = TRUE, na.strings = c("", NA, "#DIV/0!"))
```

##Cleaning up data

The training data is contained in pml-training.csv. The data has ~19000 observations and 160 variables.

First thing that we are able to notice in the data that there is a lot of mostly empty columns. Also, some rows have "#DIV/0!" fields, which I interpreted as division by zero so I treated them as missing values. If we wanted to impute the missing data, then we might need to treat division by zero differently, but since I decided to not use columns with missing data, treating divison by zero as NA is safe to do.
I removed all columns which have more than 30% NA values.

By examining the pml-training.csv, we can establish that it is very neatly ordered by person name and timestamps. This kind of data ordering will cause over-optimistic results on training set and will produce worse results on general data - it will underestimate out-of-sample errors.
I removed the following columns:

```{r, echo=FALSE}
colsToRemove <- c("X", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
colsToRemove
```

Since the testing file, pml-testing.csv, contains the same persons covered in the training file, I decided to keep the "user_name" column because it might be significant for the results in this test file.
However, for a different (general) test file with different people, "user_name" should not be used for the reasons mentioned above.

```{r clean up data, echo=FALSE}
##removing columns that are mostly null
mostlyNullColumns <- colSums(is.na(trainingData)) >= 0.3 * nrow(trainingData)

trainingData <- trainingData[, !mostlyNullColumns]
testingData <- testingData[, !mostlyNullColumns]

##removing columns that relate to timeframe, person name and ordinal number of the row.
trainingData <- trainingData[, -which(names(trainingData) %in% colsToRemove)]
testingData <- testingData[, -which(names(testingData) %in% colsToRemove)]
```

After these actions, the number of columns in the training (and test) data is:

```{r}
ncol(trainingData)
```

This number is at this point still too big - we need to reduce it more.

##Training

I have separated the data from pml-training into two sets - training and validation set. 75% of the training data is in the training set. 
Since the training set is very big and because it takes a long time to train the predictors on large datasets, I've made an exploratory dataset which has only 10% of the training data. The exploratory dataset is separated into training and test sets (70:30). The exploratory training set is sufficiently small to try machine learning algorithms on it:

```{r splitting of training data, include=FALSE}
set.seed(21128)
library(caret)
##'testing' is data to be used with cross-validation. It will not be examined.
inTrain <- createDataPartition(y=trainingData$classe,p=0.75, list=FALSE)
training <- trainingData[inTrain, ]
testing <- trainingData[-inTrain, ]

##I am taking a small subset of training data for exploratory (initial) training.
inExp <- createDataPartition(y=training$classe,p=0.1, list=FALSE)
exp <- training[inExp, ]
inExpTrain <- createDataPartition(y=exp$classe,p=0.7, list=FALSE)
expTrain <- exp[inExpTrain, ]
expTest  <- exp[-inExpTrain, ]
```

```{r number of expTrain rows}
nrow(expTrain)
```

Random forest training is used on exploratory training set.

```{r exploratory training, include=FALSE}
library(randomForest)
rforest <- randomForest(classe~ ., data=expTrain)
```

```{r exploratory training display, echo = FALSE}
rforest
```

We are getting the following predictions on exploratory test set:
```{r exploratory results, echo=FALSE}
pred <- predict(rforest, expTest)
table(pred, expTest$classe)
```

I decided to use the 20 most significant variables (obtained by sorting varImp()) as the significant columns in the (main) training.

```{r reducing columns, fig.width=5, fig.height=5, echo=FALSE}
varImpPlot(rforest)
importance <- varImp(rforest)
importantCols <- rownames(importance)[order(importance$Overall, decreasing=TRUE)[1:20]]
importantCols
```

Now, the whole training data is reduced to only 20 most significant columns. Subsequently, random forest is performed on the training data.

```{r significant variables, echo=FALSE}
importantCols <- append(importantCols, "classe")
trainingData <- trainingData[, which(names(trainingData) %in% importantCols)]
testingData <- testingData[, which(names(testingData) %in% importantCols)]

training <- training[, which(names(training) %in% importantCols)]
testing <- testing[, which(names(testing) %in% importantCols)]
```

```{r training, include=FALSE}
rforest <- randomForest(classe~ ., data=training)
```

```{r training display, echo=FALSE}
rforest
```

##Results

As we can see in the previous listing, out-of-bag (OOB) error estimate is less than 1%.

Random forest gives the following results on validation data:

```{r validation results}
pred <- predict(rforest, testing)
table(pred, testing$classe)
testing$right <- testing$classe == pred
sum(testing$right) / nrow(testing)
```

Now, we perform random forest on data from pml-testing.cv:

```{r test results}
finalPred <- predict(rforest, testingData)
finalPred
```

The results obtained match the ones expected by Submission part of course project.
<br><br>
